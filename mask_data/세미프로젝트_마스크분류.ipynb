{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마스크 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 경로\n",
    "train_dir = 'https://github.com/Wildturkeyy/Deep_learning.git'\n",
    "\n",
    "# 테스트 데이터 경로\n",
    "test_dir = 'mask_dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 가로 세로 \n",
    "im_width = 224\n",
    "im_height = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용된 이미지를 읽을 객체\n",
    "\n",
    "# 실행할 때마다 변형된 이미지를 리턴해서 이미지가 많은 것 같은 효과를 줌\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rotation_range = 180, # 회전 최대 180도\n",
    "            width_shift_range = 0.2, # 좌우 이동 최대 이미지 가로 사이즈 20%\n",
    "            height_shift_range = 0.2, # 상하 이동 최대 이미지 세로 사이즈 20%\n",
    "            horizontal_flip=True, # 좌우 반전 실행\n",
    "            vertical_flip=True, # 상하 반전 실행\n",
    "            rescale=1./225, # 이미지를 255로 나눠서 0~1 사이값으로 반환\n",
    "            brightness_range=[0.5 , 1.2], # 이미지 밝기 조정\n",
    "            # 완전 어두운 이미지 1, 원본밝기 0.5 : 원본 50%밝기, 1.2 : 원본보다 20%밝은 이미\n",
    "            zoom_range = [0.8, 1.2] # 이미지 확대 0.8 원본의 80%확대, 1.2:원본의 120% 확대\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] 파일 이름, 디렉터리 이름 또는 볼륨 레이블 구문이 잘못되었습니다: 'https://github.com/Wildturkeyy/Deep_learning.git'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20200\\4056910287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mclass_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m         )\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\keras_preprocessing\\image\\directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] 파일 이름, 디렉터리 이름 또는 볼륨 레이블 구문이 잘못되었습니다: 'https://github.com/Wildturkeyy/Deep_learning.git'"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(im_height, im_width),\n",
    "            batch_size=64,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증에 사용될 이미지를 읽을 객체\n",
    "# 테스트에서는 이미지를 변경할 필요가 없으므로 \n",
    "# imagedatagenerator에 값을 주지 않음\n",
    "# 이미지가 변형되지 않고 같은 이미지 리턴\n",
    "# 이미지를 255로 나눠서 0~1 사이값으로 변환\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 419 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "            test_dir,\n",
    "            target_size=(im_height, im_width),\n",
    "            batch_size=64,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 데이터의 전체 이미지 개수를 리턴\n",
    "train_generator.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한번에 리턴할 이미지의 개수\n",
    "train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터의 전체 이미지 개수 리턴\n",
    "test_generator.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator.next()\n",
    "# train_generator.batch_size에 설정된 이미지 128??개를 리턴해서 img에 대입\n",
    "# 이미지의 label을 label에 대입\n",
    "img, label = train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding되어있음\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# VGG16 구조를 갖는 model 생성\n",
    "conv_layers = VGG16(\n",
    "            weights='imagenet', # 이미지넷 대회에서 학습한 필터의 weight 그대로 사용\n",
    "            include_top=False, # 맨 마지막의 100개로 분류하는 선형회귀 삭제\n",
    "            input_shape=(im_height, im_width, 3) # 이미지의 im_width 가로 im_height 세로 3컬러\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지넷 대회의 필터를 그대로 사용\n",
    "for layer in conv_layers.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    # 해당 레이어의 weight는 수정하지 않고 이미지넷 대회의 값 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값을 읽어서 예측할 Sequential 객체 생성\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 대입\n",
    "model.add(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀를 하기 위해서 합성곱 연산을 수행한 결과를 1차원 배열로 변환\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: 선형회귀를 수행할 객체\n",
    "# Dense(출력데이터의 칸의 수): 출력 데이터는 y_h1이고 칸의 수는 512이므로\n",
    "# Dense(512)???\n",
    "model.add(Dense(512, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체에서 임의의 50%만 학습\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 27,692,355\n",
      "Trainable params: 12,977,667\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDecent를 이용해서 w0 b0, w1, b1를 찾는 방법 설정\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-5), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시 가장 정확도가 높은 모델을 저장하도록 설정\n",
    "cb_checkpoint = ModelCheckpoint(\n",
    "                # filepath='./check/', # 학습 진행시 가장 정확도가 높은 모델 저장할 경로\n",
    "                filepath='./check/saved_model.pb', # 학습 진행시 가장 정확도가 높은 모델 저장할 경로  '{epoch:02d}-{val_loss:.5f}.h5'\n",
    "                monitor = 'val_acc', # 저장할 조건 val_acc(테스트 데이터의 정확도)가 가장 높은 모델 저 \n",
    "                vervose = 1, # 함수의 진행 과정 출력\n",
    "                save_best_only=True # 가장 정확도가 높은 모델 1개만 저장.\n",
    ")               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.9837 - acc: 0.5009 Epoch 1/50\n",
      "26/26 [==============================] - 234s 9s/step - loss: 0.9773 - acc: 0.5063 - val_loss: 0.7742 - val_acc: 0.7088\n",
      "Epoch 2/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.9485 - acc: 0.5317 Epoch 1/50\n",
      "26/26 [==============================] - 238s 9s/step - loss: 0.9485 - acc: 0.5322 - val_loss: 0.7390 - val_acc: 0.7279\n",
      "Epoch 3/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.8891 - acc: 0.5818 Epoch 1/50\n",
      "26/26 [==============================] - 239s 9s/step - loss: 0.8888 - acc: 0.5817 - val_loss: 0.7117 - val_acc: 0.7351\n",
      "Epoch 4/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.8582 - acc: 0.5975 Epoch 1/50\n",
      "26/26 [==============================] - 243s 9s/step - loss: 0.8560 - acc: 0.5992 - val_loss: 0.6756 - val_acc: 0.7351\n",
      "Epoch 5/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.8655 - acc: 0.5931 Epoch 1/50\n",
      "26/26 [==============================] - 229s 9s/step - loss: 0.8618 - acc: 0.5955 - val_loss: 0.6544 - val_acc: 0.7446\n",
      "Epoch 6/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.8265 - acc: 0.6201 Epoch 1/50\n",
      "26/26 [==============================] - 227s 9s/step - loss: 0.8228 - acc: 0.6251 - val_loss: 0.6382 - val_acc: 0.7494\n",
      "Epoch 7/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.7811 - acc: 0.6514 Epoch 1/50\n",
      "26/26 [==============================] - 225s 9s/step - loss: 0.7780 - acc: 0.6552 - val_loss: 0.6149 - val_acc: 0.7422\n",
      "Epoch 8/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.7879 - acc: 0.6502 Epoch 1/50\n",
      "26/26 [==============================] - 225s 9s/step - loss: 0.7872 - acc: 0.6504 - val_loss: 0.6085 - val_acc: 0.7661\n",
      "Epoch 9/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.7526 - acc: 0.6708 Epoch 1/50\n",
      "26/26 [==============================] - 224s 9s/step - loss: 0.7520 - acc: 0.6733 - val_loss: 0.5851 - val_acc: 0.7613\n",
      "Epoch 10/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.7666 - acc: 0.6690 Epoch 1/50\n",
      "26/26 [==============================] - 237s 9s/step - loss: 0.7669 - acc: 0.6691 - val_loss: 0.5669 - val_acc: 0.7804\n",
      "Epoch 11/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.7172 - acc: 0.6884 Epoch 1/50\n",
      "26/26 [==============================] - 231s 9s/step - loss: 0.7189 - acc: 0.6908 - val_loss: 0.5439 - val_acc: 0.7924\n",
      "Epoch 12/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.7294 - acc: 0.6759 Epoch 1/50\n",
      "26/26 [==============================] - 222s 9s/step - loss: 0.7265 - acc: 0.6775 - val_loss: 0.5433 - val_acc: 0.7828\n",
      "Epoch 13/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.7061 - acc: 0.6984 Epoch 1/50\n",
      "26/26 [==============================] - 222s 9s/step - loss: 0.7034 - acc: 0.6992 - val_loss: 0.5153 - val_acc: 0.8162\n",
      "Epoch 14/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.6788 - acc: 0.7141 Epoch 1/50\n",
      "26/26 [==============================] - 3249s 125s/step - loss: 0.6777 - acc: 0.7143 - val_loss: 0.5186 - val_acc: 0.8067\n",
      "Epoch 15/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6760 - acc: 0.7072 Epoch 1/50\n",
      "26/26 [==============================] - 238s 9s/step - loss: 0.6774 - acc: 0.7071 - val_loss: 0.4948 - val_acc: 0.8305\n",
      "Epoch 16/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6618 - acc: 0.7275 Epoch 1/50\n",
      "26/26 [==============================] - 234s 9s/step - loss: 0.6686 - acc: 0.7233 - val_loss: 0.4828 - val_acc: 0.8377\n",
      "Epoch 17/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6507 - acc: 0.7350 Epoch 1/50\n",
      "26/26 [==============================] - 241s 9s/step - loss: 0.6541 - acc: 0.7342 - val_loss: 0.4780 - val_acc: 0.8305\n",
      "Epoch 18/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6549 - acc: 0.7275 Epoch 1/50\n",
      "26/26 [==============================] - 229s 9s/step - loss: 0.6531 - acc: 0.7275 - val_loss: 0.4690 - val_acc: 0.8353\n",
      "Epoch 19/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6477 - acc: 0.7260 Epoch 1/50\n",
      "26/26 [==============================] - 237s 9s/step - loss: 0.6495 - acc: 0.7245 - val_loss: 0.4623 - val_acc: 0.8329\n",
      "Epoch 20/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6214 - acc: 0.7354 Epoch 1/50\n",
      "26/26 [==============================] - 243s 9s/step - loss: 0.6234 - acc: 0.7372 - val_loss: 0.4601 - val_acc: 0.8496\n",
      "Epoch 21/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.6360 - acc: 0.7411 Epoch 1/50\n",
      "26/26 [==============================] - 225s 9s/step - loss: 0.6303 - acc: 0.7432 - val_loss: 0.4465 - val_acc: 0.8473\n",
      "Epoch 22/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.6169 - acc: 0.7505 Epoch 1/50\n",
      "26/26 [==============================] - 218s 8s/step - loss: 0.6190 - acc: 0.7498 - val_loss: 0.4405 - val_acc: 0.8520\n",
      "Epoch 23/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.6151 - acc: 0.7524 Epoch 1/50\n",
      "26/26 [==============================] - 219s 8s/step - loss: 0.6148 - acc: 0.7529 - val_loss: 0.4324 - val_acc: 0.8544\n",
      "Epoch 24/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.5836 - acc: 0.7586 Epoch 1/50\n",
      "26/26 [==============================] - 225s 9s/step - loss: 0.5844 - acc: 0.7601 - val_loss: 0.4291 - val_acc: 0.8711\n",
      "Epoch 25/50\n",
      "25/26 [===========================>..] - ETA: 7s - loss: 0.6166 - acc: 0.7542 Epoch 1/50\n",
      "26/26 [==============================] - 230s 9s/step - loss: 0.6158 - acc: 0.7535 - val_loss: 0.4243 - val_acc: 0.8496\n",
      "Epoch 26/50\n",
      "25/26 [===========================>..] - ETA: 6s - loss: 0.5719 - acc: 0.7649 Epoch 1/50\n",
      "26/26 [==============================] - 225s 9s/step - loss: 0.5730 - acc: 0.7625 - val_loss: 0.4238 - val_acc: 0.8473\n",
      "Epoch 27/50\n",
      " 4/26 [===>..........................] - ETA: 2:31 - loss: 0.5915 - acc: 0.7695"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18352\\3046328188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcb_checkpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1015\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tensorflow1_env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.fit(train_generator, epoch=50) : train_generator를 이용해서 이미지를 리턴받음\n",
    "# epoch를 진행할때 마다 증강된 이미지를 리턴\n",
    "# 64개씩(batch_size) 50번 반복해서 학습\n",
    "\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        validation_data = test_generator,\n",
    "        callbacks=[cb_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# 저장한 모델을 읽어서 best_model 변수에 대입\n",
    "best_model = keras.models.load_model('./check/saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model을 이용해 정확도 계산\n",
    "best_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_generator.next()\n",
    "# test_generator.batch_size에 설정된 이미지 50개를 리턴해서 X_test에 대입\n",
    "# 이미지의 label을  y_test에 대입\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.predict(X_test) :\n",
    "# X_test가 어떤 이미지인지 분류\n",
    "predict = best_model.predict(X_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(predict, 1) : predict의 최대값의 인덱스를 리턴\n",
    "predict01 = np.argmax(predict, 1)\n",
    "predict01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test 값도 최대값의 인덱스를 리턴\n",
    "target = np.argmax(y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict02 = (predict01 == target)\n",
    "predict02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True는 1로 False는 0으로 변환해서 합을 계산\n",
    "np.sum(predict02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "acc = np.sum(predict02)/len(predict02)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블의 이름을 리턴\n",
    "train_generator.class_indices\n",
    "# train_generator.class_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 레이블의 이름을 리스트로 변환해서 custom_labels에 대입\n",
    "custom_labels = list(train_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_labels[0])\n",
    "print(custom_labels[1])\n",
    "print(custom_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 출력할 객체\n",
    "fig = plt.figrue(figsize=(30, 40))\n",
    "\n",
    "for i in range(64):\n",
    "    # 8줄 8칸으로 나누고 i+1번째 이미지를 그림\n",
    "    subplot = fig.add_subplot(8, 8, i+1)\n",
    "\n",
    "    #subplot.set_xticks([]) : 그래프의 x축을 설정\n",
    "    # 데이터가 없으므로 x축에 아무 데이터도 출력 안됨\n",
    "    subplot.set_xticks([])\n",
    "    subplot.set_yticks([])\n",
    "\n",
    "    # 예측값(숫자)을 predict_num에 대입\n",
    "    predict_num = predict01[i]\n",
    "    # 예측값에 해당하는 문자를 리턴\n",
    "    predict_str = custom_labels[predict_num]\n",
    "\n",
    "    # 실제 값 (숫자) 리턴\n",
    "    target_num = target[i]\n",
    "    # 실제 값에 해당하는 문자 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3a336bbbac7a5316600585707c60468fe6500cf7c2d174bf746727352f6e866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
